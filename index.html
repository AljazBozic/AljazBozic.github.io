<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Aljaž Božič</title>

  <meta name="description" content="The personal website of Aljaž Božič, Ph.D. candidate at TU Munich.">
  <meta name="author" content="Aljaž Božič">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="main_page/stylesheet.css">
</head>

<body>
  <table
    style="width:100%;max-width:860px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%; width:63%; vertical-align:middle; text-align: justify;">
                  <p style="text-align:center">
                    <name>Aljaž Božič</name>
                  </p>
                  <p>
                    I am a Research Scientist at Meta Reality Labs Research.
                  </p>
                  <p>
                    I did my Ph.D. with <a
                      href="http://niessnerlab.org/members/matthias_niessner/profile.html">Prof. Matthias
                      Nießner</a> at <a href="http://niessnerlab.org/team.html">Visual
                      Computing Group</a>, at the <a href="https://www.tum.de/">Technical University of
                      Munich</a>.
                  </p>
                  <p>
                    I received a Master's Degree in Computer Science from the <a
                      href="https://www.tum.de/">Technical University of Munich</a> and a Bachelor's Degree in
                    Mathematics from the <a href="https://www.uni-lj.si/eng/">University of
                      Ljubljana</a>.
                  </p>
                  <p>
                    I'm interested in computer vision, graphics and AI.

                    My research is mostly focused on neural rendering and generative AI, especially when it comes to modelling 3D deformable objects, with applications in VR/AR, robotics, etc.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:aljaz.bozic@tum.de">Email</a> &nbsp|&nbsp
                    <a href="https://scholar.google.com/citations?user=92YuESsAAAAJ&hl=en&oi=ao">Google
                      Scholar</a>
                    &nbsp|&nbsp
                    <a href="https://twitter.com/BozicAljaz">Twitter</a> &nbsp|&nbsp
                    <a href="https://github.com/AljazBozic">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="main_page/images/AljazBozic.jpg"><img style="width:100%;max-width:100%"
                      alt="profile photo" src="main_page/images/AljazBozic_Circle.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding-left:20px; padding-top:20px; width:100%; vertical-align:middle">
                  <heading>Research</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; text-align: justify;">
            <tbody>
              <!-- SceNeRFlow: Time-Consistent Reconstruction of General Dynamic Scenes -->
              <tr onmouseout="scenerflow_stop()" onmouseover="scenerflow_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <a href="https://vcai.mpi-inf.mpg.de/projects/scenerflow/">
                      <div class="two" id="scenerflow_image" style="opacity: 0;">
                        <img src="main_page/images/tretschk2024scenerflow.png" width="160">
                      </div>
                      <img src="main_page/images/tretschk2024scenerflow_before.png" width="160">
                    </a>
                  </div>

                  <script type="text/javascript">
                    function scenerflow_start() {
                      document.getElementById('scenerflow_image').style.opacity = 1;
                    }
                    function scenerflow_stop() {
                      document.getElementById('scenerflow_image').style.opacity = 0;
                    }
                    scenerflow_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://vcai.mpi-inf.mpg.de/projects/scenerflow/">
                  <papertitle>SceNeRFlow: Time-Consistent Reconstruction of General Dynamic Scenes</papertitle>
                  </a>
                  <br>

                  Edith Tretschk, Vladislav Golyanik, Michael Zollhöfer, <b>Aljaž Božič</b>, Christoph Lassner, Christian Theobalt<br>

                  <b>3DV 2024</b><br>

                  <a href="https://arxiv.org/abs/2308.08258">paper</a> |
                  <a href="https://vcai.mpi-inf.mpg.de/projects/scenerflow/data/scenerflow.mp4">video</a> |
                  <a href="main_page/bibtex/tretschk2024scenerflow.bib" type="text/html">bibtex</a>

                  <p>
                    We propose SceNeRFlow to reconstruct a general, non-rigid scene in a time-consistent manner.
                    Our dynamic-NeRF method takes multi-view RGB videos and background images from static cameras
                    with known camera parameters as input. We use a backwards deformation model and can accurately model
                    long studio-scale motions.
                  </p>

                </td>
              </tr>

              <!-- A Local Appearance Model for Volumetric Capture of Diverse Hairstyles -->
              <tr onmouseout="lochamo_stop()" onmouseover="lochamo_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <a href="https://ziyanw1.github.io/LocHaMo/">
                      <div class="two" id="lochamo_image" style="opacity: 0;">
                        <video width="160" height="160" muted="" autoplay="" loop="">
                          <source src="main_page/images/wang2024lochamo.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                      </div>
                      <img src="main_page/images/wang2024lochamo_before.png" width="160">
                    </a>
                  </div>

                  <script type="text/javascript">
                    function lochamo_start() {
                      document.getElementById('lochamo_image').style.opacity = 1;
                    }
                    function lochamo_stop() {
                      document.getElementById('lochamo_image').style.opacity = 0;
                    }
                    lochamo_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://ziyanw1.github.io/LocHaMo/">
                  <papertitle>A Local Appearance Model for Volumetric Capture of Diverse Hairstyles</papertitle>
                  </a>
                  <br>

                  Ziyan Wang, Giljoo Nam, <b>Aljaž Božič</b>, Chen Cao, Jason Saragih, Michael Zollhöfer, Jessica Hodgins<br>

                  <b>3DV 2024</b><br>

                  <a href="https://arxiv.org/abs/2312.08679">paper</a> |
                  <a href="main_page/bibtex/wang2024lochamo.bib" type="text/html">bibtex</a>

                  <p>
                    We present a novel local appearance model that is capable of capturing the
                    photorealistic appearance of diverse hairstyles in a volumetric way. Our method
                    leverages the local similarity across different hairstyles and learns a universal
                    hair appearance prior from multi-view captures of hundreds of people.
                  </p>

                </td>
              </tr>

              <!-- VR-NeRF: High-Fidelity Virtualized Walkable Spaces -->
              <tr onmouseout="vrnerf_stop()" onmouseover="vrnerf_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <a href="https://vr-nerf.github.io/">
                      <div class="two" id="vrnerf_image" style="opacity: 0;">
                        <img src="main_page/images/xu2023vrnerf.png" width="160">
                      </div>
                      <img src="main_page/images/xu2023vrnerf_before.png" width="160">
                    </a>
                  </div>

                  <script type="text/javascript">
                    function vrnerf_start() {
                      document.getElementById('vrnerf_image').style.opacity = 1;
                    }
                    function vrnerf_stop() {
                      document.getElementById('vrnerf_image').style.opacity = 0;
                    }
                    vrnerf_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://vr-nerf.github.io/">
                  <papertitle>VR-NeRF: High-Fidelity Virtualized Walkable Spaces</papertitle>
                  </a>
                  <br>

                  Linning Xu, Vasu Agrawal, William Laney, Tony Garcia, Aayush Bansal, Changil Kim, Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder, <b>Aljaž Božič</b>, Dahua Lin, Michael Zollhöfer, Christian Richardt<br>

                  <b>SIGGRAPH Asia 2023</b><br>

                  <a href="https://arxiv.org/abs/2311.02542">paper</a> |
                  <a href="https://www.youtube.com/watch?v=9Q8_FssFQP4">video</a> |
                  <a href="main_page/bibtex/xu2023vrnerf.bib" type="text/html">bibtex</a>

                  <p>
                    VR-NeRF brings high-fidelity walkable spaces to real-time virtual reality.
                    Our “Eyeful Tower” multi-camera rig captures spaces with high image resolution
                    and dynamic range that approach the limits of the human visual system.
                  </p>

                </td>
              </tr>

              <!-- Learning Neural Duplex Radiance Fields for Real-Time View Synthesis -->
              <tr onmouseout="ndrf_stop()" onmouseover="ndrf_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <a href="http://raywzy.com/NDRF/">
                      <div class="two" id="ndrf_image" style="opacity: 0;">
                        <img src="main_page/images/wan2023ndrf.png" width="160">
                      </div>
                      <img src="main_page/images/wan2023ndrf_before.png" width="160">
                    </a>
                  </div>

                  <script type="text/javascript">
                    function ndrf_start() {
                      document.getElementById('ndrf_image').style.opacity = 1;
                    }
                    function ndrf_stop() {
                      document.getElementById('ndrf_image').style.opacity = 0;
                    }
                    ndrf_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="http://raywzy.com/NDRF/">
                  <papertitle>Learning Neural Duplex Radiance Fields for Real-Time View Synthesis</papertitle>
                  </a>
                  <br>

                  Ziyu Wan, Christian Richardt, <b>Aljaž Božič</b>, Chao Li, Vijay Rengarajan, Seonghyeon Nam, Xiaoyu Xiang, Tuotuo Li, Bo Zhu, Rakesh Ranjan, Jing Liao<br>

                  <b>CVPR 2023</b><br>

                  <a href="https://arxiv.org/abs/2304.10537">paper</a> |
                  <a href="https://www.youtube.com/watch?v=0xtbTClz7r0">video</a> |
                  <a href="main_page/bibtex/wan2023ndrf.bib" type="text/html">bibtex</a>

                  <p>
                    We introduce a novel approach to distill and bake NeRFs into highly efficient mesh-based
                    neural representations that are fully compatible with the massively parallel graphics
                    rendering pipeline. We represent scenes as neural radiance features encoded on a two-layer duplex
                    mesh, which effectively overcomes the inherent inaccuracies in 3D surface reconstruction.
                  </p>

                </td>
              </tr>

              <!-- Neural Lens Modeling -->
              <tr onmouseout="neurolens_stop()" onmouseover="neurolens_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <a href="https://neural-lens.github.io/">
                      <div class="two" id="neurolens_image" style="opacity: 0;">
                        <img src="main_page/images/xian2023neurolens.png" width="160">
                      </div>
                      <img src="main_page/images/xian2023neurolens_before.png" width="160">
                    </a>
                  </div>

                  <script type="text/javascript">
                    function neurolens_start() {
                      document.getElementById('neurolens_image').style.opacity = 1;
                    }
                    function neurolens_stop() {
                      document.getElementById('neurolens_image').style.opacity = 0;
                    }
                    neurolens_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://neural-lens.github.io/">
                  <papertitle>Neural Lens Modeling</papertitle>
                  </a>
                  <br>

                  Wenqi Xian, <b>Aljaž Božič</b>, Noah Snavely, Christoph Lassner<br>

                  <b>CVPR 2023</b><br>

                  <a href="https://arxiv.org/abs/2304.04848">paper</a> |
                  <a href="main_page/bibtex/xian2023neurolens.bib" type="text/html">bibtex</a>

                  <p>
                    We propose NeuroLens, a neural lens model for distortion and vignetting that can be
                    used for point projection and ray casting and can be optimized through both operations.
                    This means that it can (optionally) be used to perform pre-capture calibration using
                    classical calibration targets, and can later be used to perform calibration or refinement
                    during 3D reconstruction, e.g., while optimizing a radiance field.
                  </p>

                </td>
              </tr>

              <!-- Neural Assets: Volumetric Object Capture and Rendering for Interactive Environments -->
              <tr onmouseout="neuralassets_stop()" onmouseover="neuralassets_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <a href="#">
                      <div class="two" id="neuralassets_image" style="opacity: 0;">
                        <img src="main_page/images/bozic2022neuralassets.png" width="160">
                      </div>
                      <img src="main_page/images/bozic2022neuralassets_before.png" width="160">
                    </a>
                  </div>

                  <script type="text/javascript">
                    function neuralassets_start() {
                      document.getElementById('neuralassets_image').style.opacity = 1;
                    }
                    function neuralassets_stop() {
                      document.getElementById('neuralassets_image').style.opacity = 0;
                    }
                    neuralassets_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="#">
                  <papertitle>Neural Assets: Volumetric Object Capture and Rendering for Interactive Environments</papertitle>
                  </a>
                  <br>

                  <b>Aljaž Božič</b>, Denis Gladkov, Luke Doukakis, Christoph Lassner<br>

                  <b>arXiv 2022</b><br>

                  <a href="https://arxiv.org/abs/2212.06125">paper</a> |
                  <a href="https://www.facebook.com/watch/?v=494508785917844">video</a> |
                  <a href="main_page/bibtex/bozic2022neuralassets.bib" type="text/html">bibtex</a>

                  <p>
                    We propose a novel neural representation for capturing real-world objects in everyday
                    environments, featuring photorealistic appearance and volumetric effects, such as
                    translucent object parts. Our real-time model architecture is transpiled into efficient
                    shader code seamlessly integrated into a modern game engine, supporting object interactions,
                    shadows, etc.
                  </p>

                </td>
              </tr>

              <!-- RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering -->
              <tr onmouseout="rcmvsnet_stop()" onmouseover="rcmvsnet_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <a href="https://boese0601.github.io/rc-mvsnet">
                        <div class="two" id="rcmvsnet_image" style="opacity: 0;">
                          <img src="main_page/images/chang2022rcmvsnet.jpg" width="160">
                        </div>
                        <img src="main_page/images/chang2022rcmvsnet_before.jpg" width="160">
                      </a>
                    </div>

                    <script type="text/javascript">
                      function rcmvsnet_start() {
                        document.getElementById('rcmvsnet_image').style.opacity = 1;
                      }
                      function rcmvsnet_stop() {
                        document.getElementById('rcmvsnet_image').style.opacity = 0;
                      }
                      rcmvsnet_stop();
                    </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://boese0601.github.io/rc-mvsnet">
                  <papertitle>RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering</papertitle>
                  </a>
                  <br>

                  Di Chang, <b>Aljaž Božič</b>, T. Zhang, Q. Yan, Y. Chen, S. Süsstrunk, Matthias Nießner<br>

                  <b>ECCV 2022</b><br>

                  <a href="https://arxiv.org/abs/2203.03949">paper</a> |
                  <a href="https://github.com/Boese0601/RC-MVSNet">code</a> |
                  <a href="https://www.youtube.com/watch?v=I_Q47TxTLbs">video</a> |
                  <a href="main_page/bibtex/chang2022rc.bib" type="text/html">bibtex</a>

                  <p>
                    We introduce RC-MVSNet, an unsupervised multi-view stereo reconstruction approach
                    that leverages NeRF-like rendering to generate consistent photometric supervision
                    for non-Lambertian surfaces, and propose an improved Gaussian-Uniform sampling
                    to overcome occlusion artifacts present in existing approaches.
                  </p>

                </td>
              </tr>

              <!-- TransformerFusion: Monocular RGB Scene Reconstruction using Transformers -->
              <tr onmouseout="transformerfusion_stop()" onmouseover="transformerfusion_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                    <a href="https://aljazbozic.github.io/transformerfusion">
                        <div class="two" id="transformerfusion_image" style="opacity: 0;">
                        <video width="100%" height="100%" muted="" autoplay="" loop="">
                            <source src="main_page/images/bozic2021transformerfusion.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        </div>
                        <img src="main_page/images/bozic2021transformerfusion_before.jpg" width="160">
                    </a>
                    </div>

                    <script type="text/javascript">
                    function transformerfusion_start() {
                        document.getElementById('transformerfusion_image').style.opacity = 1;
                    }
                    function transformerfusion_stop() {
                        document.getElementById('transformerfusion_image').style.opacity = 0;
                    }
                    transformerfusion_stop();
                    </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://aljazbozic.github.io/transformerfusion">
                  <papertitle>TransformerFusion: Monocular RGB Scene Reconstruction using Transformers</papertitle>
                  </a>
                  <br>

                  <b>Aljaž Božič</b>, Pablo Palafox, Justus Thies, Angela Dai, Matthias Nießner<br>
                  <b>NeurIPS 2021</b><br>

                  <a href="https://arxiv.org/abs/2107.02191">paper</a> |
                  <a href="https://github.com/AljazBozic/TransformerFusion">code</a> |
                  <a href="https://www.youtube.com/watch?v=LIpTKYfKSqw">video</a> |
                  <a href="main_page/bibtex/bozic2021transformerfusion.bib" type="text/html">bibtex</a>

                  <p>
                  We introduce TransformerFusion, a transformer-based 3D scene reconstruction approach.
                  The input monocular RGB video frames are fused into a volumetric feature representation
                  of the scene by a transformer network that learns to attend to the most relevant image
                  observations, resulting in an accurate online surface reconstruction.
                  </p>

                </td>
              </tr>

              <!-- NPMs: Neural Parametric Models for 3D Deformable Shapes -->
              <tr onmouseout="npms_stop()" onmouseover="npms_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                        <div class="two" id='npms_image'>
                                <a href="https://pablopalafox.github.io/npms">
                                    <video width=100% height=100% muted autoplay loop>
                                        <source src="main_page/images/palafox2021npms.mp4" type="video/mp4">
                                        Your browser does not support the video tag.
                                    </video>
                                </a>
                        </div>
                        <a href="https://pablopalafox.github.io/npms">
                            <img src="main_page/images/palafox2021npms_before.jpg" width="160">
                        </a>

                    </div>

                    <script type="text/javascript">
                        function npms_start() {
                            document.getElementById('npms_image').style.opacity = "1";
                        }

                        function npms_stop() {
                            document.getElementById('npms_image').style.opacity = "0";
                        }
                        npms_stop()
                    </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://pablopalafox.github.io/npms">
                    <papertitle>NPMs: Neural Parametric Models for 3D Deformable Shapes</papertitle>
                  </a>
                  <br>

                  Pablo Palafox, <b>Aljaž Božič</b>, Justus Thies, Matthias Nießner, Angela Dai</i><br>
                  <b>ICCV 2021</b><br>

                  <a href="https://arxiv.org/abs/2104.00702.pdf">paper</a> |
                  <a href="https://github.com/pablopalafox/npms">code</a> |
                  <a href="https://www.youtube.com/watch?v=muZXXgkkMPY">video</a> |
                  <a href="main_page/bibtex/palafox2021npms.bib" type="text/html">bibtex</a>

                  <p>
                    We propose Neural Parametric Models (NPMs), a learned alternative to traditional, parametric 3D models.
                    4D dynamics are disentangled into latent-space representations of shape and pose, leveraging the flexibility
                    of recent developments in learned implicit functions. Once learned, NPMs enable optimization over the learned
                    spaces to fit to new observations.
                  </p>

                </td>
              </tr>

              <!-- Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction -->
              <tr onmouseout="ndg_stop()" onmouseover="ndg_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <a href="https://aljazbozic.github.io/neural_deformation_graphs">
                      <div class="two" id="ndg_image" style="opacity: 0;">
                        <video width="100%" height="100%" muted="" autoplay="" loop="">
                          <source src="main_page/images/bozic2021ndg.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                      </div>
                      <img src="main_page/images/bozic2021ndg_before.jpg" width="160">
                    </a>
                  </div>

                  <script type="text/javascript">
                    function ndg_start() {
                      document.getElementById('ndg_image').style.opacity = 1;
                    }
                    function ndg_stop() {
                      document.getElementById('ndg_image').style.opacity = 0;
                    }
                    ndg_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="https://aljazbozic.github.io/neural_deformation_graphs">
                    <papertitle>Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction</papertitle>
                  </a>
                  <br>

                  <b>Aljaž Božič</b>, Pablo Palafox, Michael Zollhöfer, Justus Thies, Angela Dai, Matthias Nießner<br>
                  <b>CVPR 2021 (Oral)</b><br>

                  <a href="https://arxiv.org/abs/2012.01451.pdf">paper</a> |
                  <a href="https://github.com/AljazBozic/NeuralGraph">code</a> |
                  <a href="https://www.youtube.com/watch?v=vyq36eFkdWo">video</a> |
                  <a href="main_page/bibtex/bozic2021ndg.bib" type="text/html">bibtex</a>

                  <p>
                    We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D
                    reconstruction of non-rigid objects. Specifically, we implicitly model a deformation graph via a
                    deep neural network and empose per-frame viewpoint consistency as well as inter-frame graph and
                    surface consistency constraints in a self-supervised fashion.
                  </p>

                </td>
              </tr>

              <!-- Neural Non-Rigid Tracking -->
              <tr onmouseout="nnrt_stop()" onmouseover="nnrt_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <a href="http://niessnerlab.org/projects/bozic2020nnrt.html">
                      <div class="two" id="nnrt_image" style="opacity: 0;">
                        <img src="main_page/images/bozic2020nnrt.png" width="160">
                      </div>
                      <img src="main_page/images/bozic2020nnrt_before.png" width="160">
                    </a>
                  </div>

                  <script type="text/javascript">
                    function nnrt_start() {
                      document.getElementById('nnrt_image').style.opacity = 1;
                    }
                    function nnrt_stop() {
                      document.getElementById('nnrt_image').style.opacity = 0;
                    }
                    nnrt_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="http://niessnerlab.org/projects/bozic2020nnrt.html">
                    <papertitle>Neural Non-Rigid Tracking</papertitle>
                  </a>
                  <br>

                  <b>Aljaž Božič*</b>, Pablo Palafox*, Michael Zollhöfer, Angela Dai, Justus Thies, Matthias Nießner<br>
                  <b>NeurIPS 2020</b><br>

                  <a href="https://arxiv.org/pdf/2006.13240.pdf">paper</a> |
                  <a href="https://github.com/DeformableFriends/NeuralTracking">code</a> |
                  <a href="https://www.youtube.com/watch?v=nqYaxM6Rj8I">video</a> |
                  <a href="main_page/bibtex/bozic2020nnrt.bib" type="text/html">bibtex</a>

                  <p>
                    We introduce a novel, end-to-end learnable, differentiable non-rigid tracker that enables
                    state-of-the-art non-rigid reconstruction. By enabling gradient back-propagation through a non-rigid
                    as-rigid-as-possible optimization solver, we are able to learn correspondences in an end-to-end
                    manner such that they are optimal for the task of non-rigid tracking.
                  </p>

                </td>
              </tr>

              <!-- Learning to Optimize Non-Rigid Tracking -->
              <tr onmouseout="optnrt_stop()" onmouseover="optnrt_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <a href="http://niessnerlab.org/projects/li2020learning.html">
                      <div class="two" id="optnrt_image" style="opacity: 0;">
                        <img src="main_page/images/li2020optnrt.png" width="160">
                      </div>
                      <img src="main_page/images/li2020optnrt_before.png" width="160">
                    </a>
                  </div>

                  <script type="text/javascript">
                    function optnrt_start() {
                      document.getElementById('optnrt_image').style.opacity = 1;
                    }
                    function optnrt_stop() {
                      document.getElementById('optnrt_image').style.opacity = 0;
                    }
                    optnrt_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="http://niessnerlab.org/projects/li2020learning.html">
                    <papertitle>Learning to Optimize Non-Rigid Tracking</papertitle>
                  </a>
                  <br>

                  Yang Li, <b>Aljaž Božič</b>, Tianwei Zhang, Yanli Ji, Tatsuya Harada, Matthias Nießner<br>
                  <b>CVPR 2020 (Oral)</b><br>

                  <a href="https://arxiv.org/pdf/2003.12230.pdf">paper</a> |
                  <a href="https://www.youtube.com/watch?v=oFgMep7xzrU">video</a> |
                  <a href="main_page/bibtex/li2020learning.bib" type="text/html">bibtex</a>

                  <p>
                    We learn the tracking of non-rigid objects by differentiating through the underlying non-rigid
                    solver. Specifically, we propose ConditionNet which learns to generate a problem-specific
                    preconditioner using a large number of training samples from the Gauss-Newton update equation. The
                    learned preconditioner increases PCG’s convergence speed by a significant margin.
                  </p>

                </td>
              </tr>

              <!-- DeepDeform: Learning Non-rigid RGB-D Reconstruction with Semi-supervised Data -->
              <tr onmouseout="deepdeform_stop()" onmouseover="deepdeform_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <a href="http://niessnerlab.org/projects/bozic2020deepdeform.html">
                      <div class="two" id="deepdeform_image" style="opacity: 0;">
                        <video width="100%" height="100%" muted="" autoplay="" loop="">
                          <source src="main_page/images/bozic2020deepdeform.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                      </div>

                      <!-- <div class="two" id="deepdeform_image" style="opacity: 0;">
                        <img src="main_page/images/bozic2020deepdeform.png" width="160">
                      </div> -->
                      <img src="main_page/images/bozic2020deepdeform_before.png" width="160">
                    </a>
                  </div>

                  <script type="text/javascript">
                    function deepdeform_start() {
                      document.getElementById('deepdeform_image').style.opacity = 1;
                    }
                    function deepdeform_stop() {
                      document.getElementById('deepdeform_image').style.opacity = 0;
                    }
                    deepdeform_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="http://niessnerlab.org/projects/bozic2020deepdeform.html">
                    <papertitle>DeepDeform: Learning Non-rigid RGB-D Reconstruction with Semi-supervised Data
                    </papertitle>
                  </a>
                  <br>

                  <b>Aljaž Božič</b>, Michael Zollhöfer, Christian Theobalt, Matthias Nießner<br>
                  <b>CVPR 2020</b><br>

                  <a href="https://arxiv.org/pdf/1912.04302.pdf">paper</a> |
                  <a href="https://github.com/AljazBozic/DeepDeform">dataset</a> |
                  <a href="https://www.youtube.com/watch?v=OrHLacCDZVQ">video</a> |
                  <a href="main_page/bibtex/bozic2020deepdeform.bib" type="text/html">bibtex</a>

                  <p>
                    We present a large dataset of 400 scenes, over 390,000 RGB-D frames, and 5,533 densely aligned frame
                    pairs, and introduce a data-driven non-rigid RGB-D reconstruction approach using learned heatmap
                    correspondences, achieving state-of-the-art reconstruction results on a newly established
                    quantitative benchmark.
                  </p>

                </td>
              </tr>

              <!-- Semantic Monocular SLAM for Highly Dynamic Environments -->
              <tr onmouseout="semanticslam_stop()" onmouseover="semanticslam_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <a href="#">
                      <div class="two" id="semanticslam_image" style="opacity: 0;">
                        <img src="main_page/images/brasch2018semanticslam.png" width="160">
                      </div>
                      <img src="main_page/images/brasch2018semanticslam_before.png" width="160">
                    </a>
                  </div>

                  <script type="text/javascript">
                    function semanticslam_start() {
                      document.getElementById('semanticslam_image').style.opacity = 1;
                    }
                    function semanticslam_stop() {
                      document.getElementById('semanticslam_image').style.opacity = 0;
                    }
                    semanticslam_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="#">
                    <papertitle>Semantic Monocular SLAM for Highly Dynamic Environments</papertitle>
                  </a>
                  <br>

                  <b>Aljaž Božič*</b>, Nikolas Brasch*, Joe Lallemand, Federico Tombari<br>
                  <b>IROS 2018</b><br>

                  <a href="https://ieeexplore.ieee.org/document/8593828">paper</a> |
                  <a href="main_page/bibtex/brasch2018semanticslam.bib" type="text/html">bibtex</a>

                  <p>
                    We propose a semantic monocular SLAM framework designed to deal with highly dynamic environments,
                    combining feature-based and direct approaches to achieve robustness under challenging conditions.
                    Our approach uses deep-learned semantic information extracted from the scene to cope with outliers
                    on dynamic objects.
                  </p>

                </td>
              </tr>

              <!-- Real-time Variational Stereo Reconstruction with Applications to Large-Scale Dense SLAM -->
              <tr onmouseout="denseslam_stop()" onmouseover="denseslam_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <a href="#">
                      <div class="two" id="denseslam_image" style="opacity: 0;">
                        <img src="main_page/images/kuschk2017denseslam.png" width="160">
                      </div>
                      <img src="main_page/images/kuschk2017denseslam_before.png" width="160">
                    </a>
                  </div>

                  <script type="text/javascript">
                    function denseslam_start() {
                      document.getElementById('denseslam_image').style.opacity = 1;
                    }
                    function denseslam_stop() {
                      document.getElementById('denseslam_image').style.opacity = 0;
                    }
                    denseslam_stop();
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <a href="#">
                    <papertitle>Real-time Variational Stereo Reconstruction with Applications to Large-Scale Dense SLAM
                    </papertitle>
                  </a>
                  <br>

                  Georg Kuschk, <b>Aljaž Božič</b>, Daniel Cremers<br>
                  <b>IEEE Intelligent Vehicles Symposium (IV), 2017</b><br>

                  <a href="https://ieeexplore.ieee.org/document/7995899">paper</a> |
                  <a href="main_page/bibtex/kuschk2017denseslam.bib" type="text/html">bibtex</a>

                  <p>
                    We propose an algorithm for dense and direct large-scale visual SLAM that runs in real-time on a
                    commodity notebook. A fast variational dense 3D reconstruction algorithm was developed which
                    robustly integrates data terms from multiple images. Embedded into a keyframe-based SLAM framework
                    it enables us to densely reconstruct large scenes.
                  </p>

                </td>
              </tr>

            </tbody>
          </table>

          <table style="padding-left:20px; padding-top:20px; width:100%; vertical-align:middle">
            <tbody>
              <tr>
                <td>
                  <heading>Teaching</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="main_page/images/teaching.png" width="160">
                </td>
                <td width="75%" valign="center" style="line-height:30px;">
                  <a href="https://justusthies.github.io/posts/3D-Scanning-and-Spatial-Learning-WS19-20/">Teaching
                    Assistant, Practical Course: 3D Scanning and Spatial Learning - Winter 2019/20</a>
                  <br>
                  <a href="https://justusthies.github.io/posts/3D-Vision-SS19/">Teaching
                    Assistant, Seminar: 3D Vision Seminar - Summer 2019</a>
                  <br>
                  <a href="https://justusthies.github.io/posts/3D-Vision-WS18-19/">Teaching
                    Assistant, Seminar: 3D Vision Seminar - Winter 2018/19</a>
                  <br>
                  <a href="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-SS18/">Teaching
                    Assistant, Lecture: 3D Scanning & Motion Capture - Summer 2018</a>
                  <br>
                  <a href="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS17-18/">Teaching
                    Assistant, Lecture: 3D Scanning & Motion Capture - Winter 2017/18</a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Source code stolen from <a href="https://jonbarron.info/">Jon Barron</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
